# PR-003: Improve Test Coverage to 95%+

**Generated**: 2026-02-08
**Resolves**: ISS-007 (Test coverage has untested lines)
**Priority**: MEDIUM
**Estimated Impact**: LOW-MEDIUM (improves confidence)
**Risk**: LOW (test additions only)

## Problem Statement
While test coverage meets the 80% target at 92.99%, 65 lines remain untested across several files. Lowest coverage is `generate_telemetry_report.py` at 78.29%.

According to `.audit/analysis/gap.yml` ISS-007:
- **Impact**: LOW - Target met but room for improvement
- **Intent Source**: QA-001 (Coverage >= 80%)
- **Current State**: 92.99% coverage with 65 uncovered lines

## Current Coverage Breakdown

From `.audit/output/summary.md`:
- **actions/lib/acceptance_tracker.py**: 88.89% (80/90 lines)
- **scripts/generate_telemetry_report.py**: 78.29% (101/129 lines) - **LOWEST**
- Other scripts: 90-99% coverage

## Proposed Changes

### 1. Add Tests for `acceptance_tracker.py` (10 missing lines)
Target uncovered error handling paths:
```python
# tests/test_acceptance_tracker_improvements.py
def test_acceptance_tracker_file_not_found():
    """Test handling of missing metrics file"""
    # Test edge case where metrics file doesn't exist

def test_acceptance_tracker_invalid_json():
    """Test handling of corrupted metrics file"""
    # Test JSON parsing error handling

def test_acceptance_tracker_permission_error():
    """Test handling of write permission issues"""
    # Test file system permission errors
```

### 2. Add Tests for `generate_telemetry_report.py` (28 missing lines)
Focus on error handling and edge cases:
```python
# tests/test_generate_telemetry_report_improvements.py
def test_telemetry_report_missing_adoption_data():
    """Test report generation with incomplete adoption data"""

def test_telemetry_report_empty_metrics():
    """Test report with zero metrics entries"""

def test_telemetry_report_malformed_input():
    """Test report handles malformed JSON gracefully"""

def test_telemetry_report_cli_arguments():
    """Test CLI argument parsing and validation"""
```

### 3. Add Integration Tests for Error Paths
```python
# tests/test_integration_error_paths.py
def test_metrics_collection_failure_recovery():
    """Test system behavior when metrics collection fails"""

def test_concurrent_metrics_write():
    """Test thread-safe metrics writing"""

def test_metrics_aggregation_with_gaps():
    """Test aggregation handles missing time periods"""
```

## Files to Create
- `tests/test_acceptance_tracker_improvements.py` (new)
- `tests/test_generate_telemetry_report_improvements.py` (new)
- `tests/test_integration_error_paths.py` (new)

## Files to Modify
- None (additive only)

## Success Criteria
1. ✅ Overall coverage increases from 92.99% to 95%+
2. ✅ Lowest file coverage >= 85%
3. ✅ All existing tests still pass
4. ✅ New tests cover error paths and edge cases
5. ✅ No regression in functionality

## Verification Plan
```bash
# Run coverage report
pytest --cov=. --cov-report=term-missing --cov-report=html

# Check specific files
pytest --cov=actions.lib.acceptance_tracker --cov-report=term-missing
pytest --cov=scripts.generate_telemetry_report --cov-report=term-missing

# Verify no regressions
pytest -v

# Target: >= 95% overall coverage
```

## Rollback Plan
If new tests break CI:
```bash
git revert <commit-hash>
# Or skip specific failing tests temporarily
pytest -k "not (test_acceptance_tracker or test_telemetry)"
```

## Estimated Time
- Analyze coverage report for specific uncovered lines: 30 minutes
- Write acceptance_tracker tests: 1 hour
- Write generate_telemetry_report tests: 1.5 hours
- Write integration error path tests: 1 hour
- Testing and validation: 30 minutes
- Total: 4.5 hours

## Dependencies
- Need to generate detailed coverage report to identify exact uncovered lines
- May need to refactor code to make error paths more testable

## Testing Strategy

### Phase 1: Coverage Analysis
```bash
pytest --cov=. --cov-report=annotate
# Check generated .coverage file for specific uncovered lines
```

### Phase 2: Test Implementation Priority
1. Error handling paths (highest priority - most likely to have bugs)
2. Edge cases (medium priority - improves robustness)
3. Integration scenarios (lower priority - already covered elsewhere)

### Phase 3: Validation
- Run tests in isolation
- Run full test suite
- Verify coverage metrics

## Open Questions
1. Are some uncovered lines defensive code that's hard to test?
   - **Action**: Review and add `# pragma: no cover` if appropriate
2. Should we add mocking for file system operations?
   - **Recommendation**: Yes, use pytest tmpdir and monkeypatch
3. What's the cost/benefit of pushing from 93% to 95%?
   - **Answer**: Low cost, medium benefit - primarily catches edge cases

## Notes
- This is additive only - no changes to production code
- Focus on error handling paths (highest value)
- May uncover latent bugs in error handling
- Some lines may be unreachable defensive code (acceptable to leave uncovered)
- Target 95% is ambitious but achievable with focused effort
