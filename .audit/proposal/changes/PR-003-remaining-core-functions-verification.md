# PR-003: Remaining Core Functions Verification

**Status**: Proposed
**Priority**: 3 (Medium)
**Gap ID**: GAP-006
**Generated**: 2026-02-08T12:25:49Z

## Problem Statement

Core Functions CF-003 (Custom Review Rules Injection) and CF-006 (AI Review Metrics Tracking) are currently unverified. According to the repository's verification requirements, all Core Functions should have testable verification scenarios and execution results.

### Current State
- **CF-001**: ✅ Verified (13/13 Actions provided)
- **CF-002**: ✅ Verified (7/13 Actions with Claude CLI)
- **CF-003**: ❌ Not Verified (Custom review rules injection)
- **CF-004**: ⚠️  Failed (Dry Run - 3/13 only)
- **CF-005**: ✅ Verified (Telemetry)
- **CF-006**: ❌ Not Verified (AI review metrics tracking)

### Impact
- Cannot confirm that README.md claims match actual implementation
- Risk of documentation-implementation drift for critical features
- Missing validation for custom review rules functionality (key differentiator)

## Proposed Solution

### Phase 1: Verification Scenarios Generation

Create verification scenarios for:
1. **CF-003**: Custom Review Rules Injection
   - Input: Custom rule file (TypeScript/Python/React/Security)
   - Expected: Rules are properly injected into AI review prompt
   - Validation: Prompt contains custom rules; review output respects rules

2. **CF-006**: AI Review Metrics Tracking
   - Input: Review activity (approve/modify/reject)
   - Expected: Metrics are recorded and acceptance rate calculated
   - Validation: `metrics/review_metrics.json` is updated correctly

### Phase 2: Verification Implementation

**File**: `.audit/verification/verify_cf003_cf006.py`

```python
#!/usr/bin/env python3
"""
Verification script for CF-003 and CF-006
Generated by Repo Genesis Auditor
"""

import json
import subprocess
import tempfile
from pathlib import Path
from dataclasses import dataclass
from typing import Any

@dataclass
class VerificationResult:
    function_id: str
    scenario_name: str
    passed: bool
    actual_output: Any
    expected_output: Any
    error_message: str | None = None
    interpretation: str = ""

def verify_cf003_custom_review_rules() -> VerificationResult:
    """CF-003: カスタムレビュールール注入機能"""

    # 1. テスト用カスタムルールファイルを作成
    custom_rules = """
    rules:
      - id: "CUSTOM-001"
        severity: "error"
        message: "Must use TypeScript strict mode"
        patterns:
          - "tsconfig.json must include strict: true"
    """

    rules_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yml', delete=False)
    rules_file.write(custom_rules)
    rules_file.close()

    try:
        # 2. review-and-merge Action でカスタムルールを指定して実行
        # 注: 実際の検証はモック環境で実行
        result = subprocess.run(
            [
                "python", "-m",
                "actions.review_and_merge",  # 実際のモジュールパスに調整
                "--custom-rules", rules_file.name,
                "--dry-run"
            ],
            capture_output=True,
            text=True,
            timeout=30
        )

        # 3. 出力にカスタムルールが含まれているか検証
        actual = result.stdout
        passed = (
            "CUSTOM-001" in actual and
            "strict mode" in actual.lower()
        )

        return VerificationResult(
            function_id="CF-003",
            scenario_name="Custom Review Rules Injection",
            passed=passed,
            actual_output=actual[:200],  # 先頭200文字
            expected_output="Output containing CUSTOM-001 and strict mode",
            interpretation=(
                "カスタムレビュールールが正常に注入されている" if passed
                else "カスタムルールがレビュー出力に含まれていない"
            )
        )

    except Exception as e:
        return VerificationResult(
            function_id="CF-003",
            scenario_name="Custom Review Rules Injection",
            passed=False,
            actual_output=None,
            expected_output="Output with custom rules",
            error_message=str(e),
            interpretation="カスタムルール注入機能の実行に失敗。実装を確認必要。"
        )
    finally:
        Path(rules_file.name).unlink(missing_ok=True)

def verify_cf006_metrics_tracking() -> VerificationResult:
    """CF-006: AIレビュー品質メトリクス追跡"""

    metrics_file = Path("metrics/review_metrics.json")

    # 1. 現在のメトリクスを取得
    before_data = {}
    if metrics_file.exists():
        before_data = json.loads(metrics_file.read_text())

    before_count = before_data.get("total_reviews", 0)

    try:
        # 2. レビューデータを1件追加（シミュレーション）
        # 注: 実際の検証はテストデータを使用
        test_review = {
            "review_id": "test-verification-001",
            "action": "review-and-merge",
            "outcome": "approved",
            "timestamp": "2026-02-08T12:25:49Z",
            "suggestions_count": 3
        }

        # メトリクス収集スクリプトを実行
        result = subprocess.run(
            ["python", "scripts/collect_metrics.py", "--test-mode"],
            capture_output=True,
            text=True,
            timeout=30,
            input=json.dumps(test_review)
        )

        # 3. メトリクスファイルが更新されたか検証
        after_data = json.loads(metrics_file.read_text()) if metrics_file.exists() else {}
        after_count = after_data.get("total_reviews", 0)

        passed = (after_count > before_count) or (result.returncode == 0)

        return VerificationResult(
            function_id="CF-006",
            scenario_name="AI Review Metrics Tracking",
            passed=passed,
            actual_output=f"Review count: {before_count} → {after_count}",
            expected_output=f"Review count should increase or be recorded",
            interpretation=(
                f"メトリクス追跡機能が正常に動作（{after_count - before_count}件増加）" if passed
                else "メトリクスが記録されていない"
            )
        )

    except Exception as e:
        return VerificationResult(
            function_id="CF-006",
            scenario_name="AI Review Metrics Tracking",
            passed=False,
            actual_output=None,
            expected_output="Increased review count",
            error_message=str(e),
            interpretation="メトリクス追跡機能の実行に失敗。実装を確認必要。"
        )

def main():
    """CF-003とCF-006を検証し、レポートを生成"""

    results = [
        verify_cf003_custom_review_rules(),
        verify_cf006_metrics_tracking(),
    ]

    # レポート生成
    print("=" * 60)
    print("CORE FUNCTIONS CF-003 & CF-006 VERIFICATION REPORT")
    print("=" * 60)

    passed_count = sum(1 for r in results if r.passed)
    total_count = len(results)

    for r in results:
        status = "✅ PASS" if r.passed else "❌ FAIL"
        print(f"\n{status} [{r.function_id}] {r.scenario_name}")
        print(f"  解釈: {r.interpretation}")
        if r.error_message:
            print(f"  エラー: {r.error_message}")

    print("\n" + "=" * 60)
    print(f"SUMMARY: {passed_count}/{total_count} passed")

    if passed_count == total_count:
        print("判定: ✅ CF-003, CF-006 の検証完了")
    else:
        print("判定: ⚠️  一部の機能に問題あり")
        print("\n次のアクション:")
        for r in results:
            if not r.passed:
                print(f"  - {r.function_id}: {r.interpretation}")

    # 結果をJSONで保存
    output_path = Path(".audit/output/verification_cf003_cf006.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(
        [r.__dict__ for r in results],
        ensure_ascii=False, indent=2
    ))

    return 0 if passed_count == total_count else 1

if __name__ == "__main__":
    exit(main())
```

### Phase 3: Test Data Preparation

**Directory**: `.audit/verification/test_data/custom_rules/`

Create sample custom rule files:
- `typescript-strict.yml`
- `python-pep8.yml`
- `react-best-practices.yml`
- `security-common.yml`

### Phase 4: Execution Plan

1. Create verification script: `.audit/verification/verify_cf003_cf006.py`
2. Generate test data in `.audit/verification/test_data/custom_rules/`
3. Run verification: `python .audit/verification/verify_cf003_cf006.py`
4. Collect results and update `intent.yml` with verification status

## Expected Outcomes

### Success Criteria
- ✅ CF-003 verification shows custom rules are properly injected
- ✅ CF-006 verification shows metrics are tracked correctly
- ✅ Both Core Functions marked as `verified_status: "passed"` in `intent.yml`

### Fallback Plan
If verification fails:
1. Document specific failure reasons
2. Categorize as "partial" or "failed" in `intent.yml`
3. Create improvement proposal for next cycle

## Alternatives Considered

### Alternative 1: Skip Verification
**Rejected**: Core Functions are the repository's reason for existence. Unverified functions undermine credibility.

### Alternative 2: Manual Testing Only
**Rejected**: Manual testing is not reproducible. Automated verification scripts ensure consistency.

## Risks and Mitigations

### Risk 1: Verification may expose implementation gaps
**Mitigation**: This is the desired outcome. Finding gaps is better than hiding them.

### Risk 2: Test data may not cover edge cases
**Mitigation**: Start with basic scenarios. Expand in subsequent iterations.

## Rollback Plan

If verification scripts cause issues:
1. Scripts are in `.audit/` directory (not production code)
2. No impact on actual Actions
3. Delete `.audit/verification/verify_cf003_cf006.py` if needed

## Verification of Proposal

### How to verify this PR
1. Check `.audit/verification/verify_cf003_cf006.py` exists
2. Check `.audit/verification/test_data/custom_rules/` contains test data
3. Run: `python .audit/verification/verify_cf003_cf006.py`
4. Verify results in `.audit/output/verification_cf003_cf006.json`

### Success Metrics
- Both CF-003 and CF-006 have verification results
- `intent.yml` updated with verification status
- No regressions in existing functionality

## Related Documentation

- `README.md:65-77` - Custom review rules
- `README.md:171-217` - AI review metrics
- `instructions/review-and-merge-custom-rules.md` - Custom rules guide
- `scripts/collect_metrics.py` - Metrics collection implementation

## Estimated Effort

- Verification script development: 1 hour
- Test data preparation: 30 minutes
- Execution and analysis: 30 minutes
- **Total**: ~2 hours

## Dependencies

- None (independent task)

## Follow-up Actions

After verification:
1. Update `intent.yml` with results
2. If gaps found, create improvement proposals
3. Document lessons learned
