#!/usr/bin/env python3
"""
Core Function Verification Script
Generated by Repo Genesis Auditor v2.0

This script verifies the core functions claimed in intent.yml
by executing actual tests and measuring results.
"""

import json
import subprocess
import sys
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Any, Optional


@dataclass
class VerificationResult:
    function_id: str
    scenario_name: str
    passed: bool
    actual_output: Any
    expected_output: Any
    error_message: Optional[str] = None
    interpretation: str = ""
    evidence_files: list = None

    def __post_init__(self):
        if self.evidence_files is None:
            self.evidence_files = []


def verify_cf_001_actions_provided() -> VerificationResult:
    """
    CF-001: 13種類のAI Actionsを提供している
    """
    print("\n[CF-001] Verifying: 13 AI Actions are provided...")

    actions_dir = Path("actions")

    if not actions_dir.exists():
        return VerificationResult(
            function_id="CF-001",
            scenario_name="13種類のAI Actions提供",
            passed=False,
            actual_output="actions/ directory not found",
            expected_output="13 action.yml files",
            error_message="actions/ directory does not exist",
            interpretation="リポジトリ構造が異常",
        )

    # Count action.yml files
    action_files = list(actions_dir.glob("*/action.yml"))
    actual_count = len(action_files)

    # Expected action names from README
    expected_actions = {
        "review-and-merge",
        "spec-to-code",
        "action-fixer",
        "auto-refactor",
        "auto-document",
        "release-notes-ai",
        "auto-merge",
        "auto-rebase",
        "review-auto-merge",
        "publish-pr",
        "bulk-merge-prs",
        "bulk-rebase-prs",
        "pr-review-enqueuer",
    }

    actual_action_names = {f.parent.name for f in action_files}

    passed = actual_count == 13 and actual_action_names == expected_actions

    return VerificationResult(
        function_id="CF-001",
        scenario_name="13種類のAI Actions提供",
        passed=passed,
        actual_output=f"{actual_count} actions: {sorted(actual_action_names)}",
        expected_output=f"13 actions: {sorted(expected_actions)}",
        interpretation=(
            f"✅ 正常に{actual_count}個のActionsが提供されている"
            if passed
            else f"⚠️ 期待数13に対して{actual_count}個。"
            f"不足: {expected_actions - actual_action_names}, "
            f"過剰: {actual_action_names - expected_actions}"
        ),
        evidence_files=[str(f) for f in action_files],
    )


def verify_cf_002_documentation_exists() -> VerificationResult:
    """
    CF-002: 各Actionには利用例（examples/）と導入手順（instructions/）が存在する
    """
    print("\n[CF-002] Verifying: Instructions and examples exist for all actions...")

    actions_dir = Path("actions")
    instructions_dir = Path("instructions")
    examples_dir = Path("examples")

    # Get all action names
    action_files = list(actions_dir.glob("*/action.yml"))
    action_names = {f.parent.name for f in action_files}

    # Check instructions
    instruction_files = {
        f.stem: f for f in instructions_dir.glob("*.md") if f.stem != "README"
    }
    example_files = {f.stem: f for f in examples_dir.glob("*.yml")}

    missing_instructions = action_names - set(instruction_files.keys())
    missing_examples = action_names - set(example_files.keys())

    passed = len(missing_instructions) == 0 and len(missing_examples) == 0

    return VerificationResult(
        function_id="CF-002",
        scenario_name="instructions/とexamples/の存在確認",
        passed=passed,
        actual_output=json.dumps(
            {
                "total_actions": len(action_names),
                "instructions_count": len(instruction_files),
                "examples_count": len(example_files),
                "missing_instructions": sorted(missing_instructions),
                "missing_examples": sorted(missing_examples),
            },
            indent=2,
        ),
        expected_output=json.dumps(
            {
                "total_actions": len(action_names),
                "instructions_count": len(action_names),
                "examples_count": len(action_names),
                "missing_instructions": [],
                "missing_examples": [],
            },
            indent=2,
        ),
        interpretation=(
            f"✅ 全{len(action_names)}個のActionにドキュメントが整備されている"
            if passed
            else f"⚠️ ドキュメント不足: instructionsなし={len(missing_instructions)}, "
            f"examplesなし={len(missing_examples)}"
        ),
        evidence_files=[str(f) for f in list(instruction_files.values())[:5]]
        + [str(f) for f in list(example_files.values())[:5]],
    )


def verify_cf_003_dry_run_validation() -> VerificationResult:
    """
    CF-003: 全てのAI ActionsはDry Runモードで自動検証される
    """
    print("\n[CF-003] Verifying: Dry Run validation exists...")

    # Check if dry-run tests exist
    test_files = list(Path("tests").glob("**/test_*dry*.py"))
    dry_run_marker_files = list(Path("tests").glob("**/test*.py"))

    # Look for dry_run markers in test files
    has_dry_run_tests = False
    for test_file in dry_run_marker_files:
        content = test_file.read_text()
        if "dry_run" in content.lower() or "dry-run" in content.lower():
            has_dry_run_tests = True
            break

    # Run pytest with dry-run related tests
    try:
        result = subprocess.run(
            ["python", "-m", "pytest", "-k", "dry", "-v", "--collect-only"],
            capture_output=True,
            text=True,
            timeout=30,
        )

        dry_run_tests_collected = "test" in result.stdout.lower()
        passed = has_dry_run_tests or dry_run_tests_collected

        return VerificationResult(
            function_id="CF-003",
            scenario_name="Dry Run検証の存在",
            passed=passed,
            actual_output=f"has_dry_run_markers={has_dry_run_tests}, "
            f"pytest_dry_run_tests={dry_run_tests_collected}",
            expected_output="Dry Run検証テストが存在する",
            interpretation=(
                "✅ Dry Run検証機能が実装されている"
                if passed
                else "⚠️ Dry Run検証の明示的な実装が見つからない"
            ),
            evidence_files=test_files[:5] if test_files else [],
        )

    except Exception as e:
        return VerificationResult(
            function_id="CF-003",
            scenario_name="Dry Run検証の存在",
            passed=False,
            actual_output=None,
            expected_output="Dry Run検証テストが存在する",
            error_message=str(e),
            interpretation="Dry Run検証の確認中にエラーが発生",
        )


def verify_qa_001_test_coverage() -> VerificationResult:
    """
    QA-001: テストカバレッジ >= 70%
    """
    print("\n[QA-001] Verifying: Test coverage >= 70%...")

    try:
        # Run pytest with coverage
        result = subprocess.run(
            [
                "python",
                "-m",
                "pytest",
                "--cov=.",
                "--cov-report=json:coverage.json",
                "--cov-report=term",
                "-q",
            ],
            capture_output=True,
            text=True,
            timeout=120,
        )

        # Parse coverage from output
        output = result.stdout + result.stderr

        # Try to read coverage.json
        coverage_file = Path("coverage.json")
        if coverage_file.exists():
            with open(coverage_file) as f:
                coverage_data = json.load(f)
            total_coverage = coverage_data.get("totals", {}).get("percent_covered", 0)
        else:
            # Parse from terminal output
            for line in output.split("\n"):
                if "TOTAL" in line:
                    parts = line.split()
                    for part in parts:
                        try:
                            total_coverage = float(part.strip("%"))
                            break
                        except ValueError:
                            continue

        target_coverage = 70.0
        passed = total_coverage >= target_coverage

        return VerificationResult(
            function_id="QA-001",
            scenario_name="テストカバレッジ >= 70%",
            passed=passed,
            actual_output=f"{total_coverage:.2f}%",
            expected_output=f">= {target_coverage}%",
            interpretation=(
                f"✅ テストカバレッジ{total_coverage:.1f}%は目標{target_coverage}%を満たしている"
                if passed
                else f"❌ テストカバレッジ{total_coverage:.1f}%は目標{target_coverage}%を{target_coverage - total_coverage:.1f}%下回っている"
            ),
            evidence_files=["coverage.json"] if coverage_file.exists() else [],
        )

    except Exception as e:
        return VerificationResult(
            function_id="QA-001",
            scenario_name="テストカバレッジ >= 70%",
            passed=False,
            actual_output=None,
            expected_output=">= 70%",
            error_message=str(e),
            interpretation="カバレッジ計測中にエラーが発生",
        )


def verify_qa_004_documentation_coverage() -> VerificationResult:
    """
    QA-004: ドキュメントカバレッジ（全Actionにinstructions/とexamples/が存在）
    """
    print("\n[QA-004] Verifying: Documentation coverage...")

    # This is similar to CF-002, but let's check the actual test
    test_file = Path("tests/test_documentation_coverage.py")

    if not test_file.exists():
        return VerificationResult(
            function_id="QA-004",
            scenario_name="ドキュメントカバレッジ検証",
            passed=False,
            actual_output="tests/test_documentation_coverage.py not found",
            expected_output="ドキュメントカバレッジ検証テストが存在",
            error_message="検証テストが存在しない",
            interpretation="QA-004を検証するテストが存在しない",
        )

    try:
        result = subprocess.run(
            ["python", "-m", "pytest", str(test_file), "-v"],
            capture_output=True,
            text=True,
            timeout=30,
        )

        passed = result.returncode == 0

        return VerificationResult(
            function_id="QA-004",
            scenario_name="ドキュメントカバレッジ検証",
            passed=passed,
            actual_output="PASSED" if passed else "FAILED",
            expected_output="PASSED",
            interpretation=(
                "✅ ドキュメントカバレッジ検証テストがパスしている"
                if passed
                else "❌ ドキュメントカバレッジ検証テストが失敗している"
            ),
            evidence_files=[str(test_file)],
        )

    except Exception as e:
        return VerificationResult(
            function_id="QA-004",
            scenario_name="ドキュメントカバレッジ検証",
            passed=False,
            actual_output=None,
            expected_output="PASSED",
            error_message=str(e),
            interpretation="検証テスト実行中にエラーが発生",
        )


def main():
    """全Core Functionを検証し、レポートを生成"""

    print("=" * 80)
    print("CORE FUNCTION VERIFICATION REPORT")
    print("Generated by Repo Genesis Auditor v2.0")
    print("=" * 80)

    # Execute verifications
    results = [
        verify_cf_001_actions_provided(),
        verify_cf_002_documentation_exists(),
        verify_cf_003_dry_run_validation(),
        verify_qa_001_test_coverage(),
        verify_qa_004_documentation_coverage(),
    ]

    # Print detailed results
    print("\n" + "=" * 80)
    print("DETAILED RESULTS")
    print("=" * 80)

    passed_count = sum(1 for r in results if r.passed)
    total_count = len(results)

    for r in results:
        status = "✅ PASS" if r.passed else "❌ FAIL"
        print(f"\n{status} [{r.function_id}] {r.scenario_name}")
        print(f"  解釈: {r.interpretation}")
        if r.error_message:
            print(f"  エラー: {r.error_message}")
        print(f"  実測値: {r.actual_output}")
        print(f"  期待値: {r.expected_output}")
        if r.evidence_files:
            print(f"  エビデンス: {', '.join(r.evidence_files[:3])}")

    # Print summary
    print("\n" + "=" * 80)
    print(f"SUMMARY: {passed_count}/{total_count} passed")
    print("=" * 80)

    if passed_count == total_count:
        print("判定: ✅ リポジトリの存在意義が検証された")
        print("\n全てのCore FunctionとQuality Attributeが基準を満たしています。")
    else:
        print("判定: ❌ 存在意義の一部が未達成")
        print("\n改善が必要な項目:")
        for r in results:
            if not r.passed:
                print(f"  - {r.function_id}: {r.interpretation}")

    # Save results to JSON
    output_path = Path(".audit/output/verification_result.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert dataclass to dict for JSON serialization
    results_dict = [asdict(r) for r in results]

    output_path.write_text(
        json.dumps(results_dict, ensure_ascii=False, indent=2)
    )

    print(f"\n詳細なレポートを保存しました: {output_path}")

    sys.exit(0 if passed_count == total_count else 1)


if __name__ == "__main__":
    main()
