#!/usr/bin/env python3
"""
Verification script for CF-003 and CF-006
Generated by Repo Improvement Executor
"""

import json
import subprocess
import sys
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Any, Optional


@dataclass
class VerificationResult:
    function_id: str
    scenario_name: str
    passed: bool
    actual_output: Any
    expected_output: Any
    error_message: Optional[str] = None
    interpretation: str = ""


def verify_cf003_custom_review_rules() -> VerificationResult:
    """CF-003: カスタムレビュールール注入機能"""

    instructions_dir = Path("instructions")
    custom_rules_guide = instructions_dir / "review-and-merge-custom-rules.md"
    examples_dir = Path("examples/custom-rules")

    try:
        # 1. カスタムルールガイドの存在確認
        has_guide = custom_rules_guide.exists()

        # 2. カスタムルールテンプレートの確認
        has_templates = False
        template_count = 0
        if examples_dir.exists():
            template_files = list(examples_dir.glob("*.yml"))
            template_count = len(template_files)
            has_templates = template_count > 0

        # 3. review-and-merge Action でカスタムルール入力の確認
        action_yml = Path("actions/review-and-merge/action.yml")
        has_custom_rules_input = False
        if action_yml.exists():
            content = action_yml.read_text()
            has_custom_rules_input = "custom-rules" in content or "custom_rules" in content

        # 総合判定
        passed = has_guide and has_templates and has_custom_rules_input

        return VerificationResult(
            function_id="CF-003",
            scenario_name="Custom Review Rules Injection",
            passed=passed,
            actual_output={
                "guide_exists": has_guide,
                "template_count": template_count,
                "custom_rules_input": has_custom_rules_input
            },
            expected_output={
                "guide_exists": True,
                "template_count": ">= 1",
                "custom_rules_input": True
            },
            interpretation=(
                f"カスタムレビュールール注入機能が実装されている"
                f"(ガイド: {has_guide}, テンプレート: {template_count}個, 入力: {has_custom_rules_input})"
                if passed
                else "カスタムレビュールール機能の実装が不完全"
            )
        )

    except Exception as e:
        return VerificationResult(
            function_id="CF-003",
            scenario_name="Custom Review Rules Injection",
            passed=False,
            actual_output=None,
            expected_output="Custom rules infrastructure",
            error_message=str(e),
            interpretation="カスタムルール注入機能の検証中にエラーが発生"
        )


def verify_cf006_metrics_tracking() -> VerificationResult:
    """CF-006: AIレビュー品質メトリクス追跡"""

    metrics_file = Path("metrics/review_metrics.json")
    scripts_dir = Path("scripts")

    try:
        # 1. メトリクス収集スクリプトの確認
        collect_script = scripts_dir / "collect_metrics.py"
        calc_script = scripts_dir / "calculate_acceptance_rate.py"
        has_scripts = collect_script.exists() and calc_script.exists()

        # 2. メトリクスファイルの確認
        metrics_exist = metrics_file.exists()

        # 3. メトリクスファイルの内容確認
        metrics_data = {}
        total_reviews_count = 0
        if metrics_exist:
            try:
                content = metrics_file.read_text()
                metrics_data = json.loads(content)
                # Handle both dict and list formats
                if isinstance(metrics_data, dict):
                    total_reviews_count = metrics_data.get("total_reviews", 0)
                elif isinstance(metrics_data, list):
                    total_reviews_count = len(metrics_data)
            except (json.JSONDecodeError, TypeError):
                pass

        has_valid_structure = "reviews" in metrics_data if isinstance(metrics_data, dict) else False

        # 4. README.mdへの記載確認
        readme = Path("README.md")
        has_readme_docs = False
        if readme.exists():
            content = readme.read_text()
            has_readme_docs = "受入率" in content or "acceptance rate" in content.lower()

        # 総合判定
        passed = has_scripts and (metrics_exist or has_readme_docs)

        return VerificationResult(
            function_id="CF-006",
            scenario_name="AI Review Metrics Tracking",
            passed=passed,
            actual_output={
                "scripts_exist": has_scripts,
                "metrics_file_exists": metrics_exist,
                "metrics_structure_valid": has_valid_structure,
                "readme_documented": has_readme_docs,
                "total_reviews": total_reviews_count
            },
            expected_output={
                "scripts_exist": True,
                "metrics_file_or_docs": True
            },
            interpretation=(
                f"メトリクス追跡機能が実装されている"
                f"(スクリプト: {has_scripts}, メトリクス: {metrics_exist}, ドキュメント: {has_readme_docs})"
                if passed
                else "メトリクス追跡機能の実装が不完全"
            )
        )

    except Exception as e:
        return VerificationResult(
            function_id="CF-006",
            scenario_name="AI Review Metrics Tracking",
            passed=False,
            actual_output=None,
            expected_output="Metrics tracking infrastructure",
            error_message=str(e),
            interpretation="メトリクス追跡機能の検証中にエラーが発生"
        )


def main():
    """CF-003とCF-006を検証し、レポートを生成"""

    results = [
        verify_cf003_custom_review_rules(),
        verify_cf006_metrics_tracking(),
    ]

    # レポート生成
    print("=" * 60)
    print("CORE FUNCTIONS CF-003 & CF-006 VERIFICATION REPORT")
    print("=" * 60)

    passed_count = sum(1 for r in results if r.passed)
    total_count = len(results)

    for r in results:
        status = "✅ PASS" if r.passed else "❌ FAIL"
        print(f"\n{status} [{r.function_id}] {r.scenario_name}")
        print(f"  解釈: {r.interpretation}")
        print(f"  実測値: {r.actual_output}")
        if r.error_message:
            print(f"  エラー: {r.error_message}")

    print("\n" + "=" * 60)
    print(f"SUMMARY: {passed_count}/{total_count} passed")

    if passed_count == total_count:
        print("判定: ✅ CF-003, CF-006 の検証完了")
    else:
        print("判定: ⚠️  一部の機能に問題あり")
        print("\n次のアクション:")
        for r in results:
            if not r.passed:
                print(f"  - {r.function_id}: {r.interpretation}")

    # 結果をJSONで保存
    output_path = Path(".audit/output/verification_cf003_cf006.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert dataclass to dict for JSON serialization
    results_dict = []
    for r in results:
        result_dict = asdict(r)
        results_dict.append(result_dict)

    output_path.write_text(
        json.dumps(results_dict, ensure_ascii=False, indent=2)
    )

    print(f"\n検証結果を保存しました: {output_path}")

    return 0 if passed_count == total_count else 1


if __name__ == "__main__":
    sys.exit(main())
