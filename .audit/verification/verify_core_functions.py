#!/usr/bin/env python3
"""
Core Function Verification Script
Generated by Repo Genesis Auditor

This script verifies the core functions claimed by the repository:
- Action structure validation
- Test coverage
- Documentation completeness
- Integration test coverage
"""

import json
import re
import subprocess
import sys
from pathlib import Path
from dataclasses import dataclass
from typing import Any, List


@dataclass
class VerificationResult:
    """Verification result data class"""
    function_id: str
    scenario_name: str
    passed: bool
    actual_output: Any
    expected_output: Any
    error_message: str | None = None
    interpretation: str = ""


def run_command(cmd: List[str], cwd: Path | None = None) -> tuple[bool, str, str]:
    """Run a command and return success, stdout, stderr"""
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=cwd,
            timeout=60
        )
        return result.returncode == 0, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return False, "", "Command timed out"
    except Exception as e:
        return False, "", str(e)


def verify_action_structure() -> VerificationResult:
    """CF-STRUCT-001: å…¨Actionsã®æ§‹é€ çš„å®Œå…¨æ€§"""
    print("\nğŸ” Verifying: Action Structure (CF-STRUCT-001)")

    # Run pytest to check action structure
    success, stdout, stderr = run_command([
        "pytest", "tests/", "-v", "-k", "test_action",
        "--tb=no", "-q"
    ])

    # Parse output - look for summary line with '=' and 'passed'
    lines = stdout.split('\n')
    summary_lines = [l for l in lines if 'passed' in l and '=' in l]

    if not summary_lines:
        return VerificationResult(
            function_id="CF-STRUCT-001",
            scenario_name="å…¨Actionsã®æ§‹é€ çš„å®Œå…¨æ€§",
            passed=False,
            actual_output=stdout[:500],
            expected_output=">= 135 passed",
            error_message="Could not parse test output",
            interpretation="ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã«å¤±æ•—ã—ãŸã‹ã€å‡ºåŠ›å½¢å¼ãŒå¤‰æ›´ã•ã‚Œã¦ã„ã‚‹"
        )

    # Extract pass count using regex
    summary_line = summary_lines[-1]
    match = re.search(r'(\d+)\s+passed', summary_line)
    
    if not match:
        return VerificationResult(
            function_id="CF-STRUCT-001",
            scenario_name="å…¨Actionsã®æ§‹é€ çš„å®Œå…¨æ€§",
            passed=False,
            actual_output=summary_line,
            expected_output=">= 135 passed",
            error_message="Could not extract pass count",
            interpretation="ãƒ†ã‚¹ãƒˆçµæœã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—"
        )
    
    passed_count = int(match.group(1))

    # Expected count should be at least 140 (139 current + margin for new tests)
    passed = passed_count >= 135

    return VerificationResult(
        function_id="CF-STRUCT-001",
        scenario_name="å…¨Actionsã®æ§‹é€ çš„å®Œå…¨æ€§",
        passed=passed,
        actual_output=f"{passed_count} passed",
        expected_output=">= 135 passed",
        interpretation=(
            f"å…¨13å€‹ã®ActionãŒæ§‹é€ çš„ã«æ­£ã—ãå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ï¼ˆ{passed_count} tests passedï¼‰"
            if passed
            else f"ä¸€éƒ¨ã®Actionã§æ§‹é€ ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹ï¼ˆ{passed_count}/140 passedï¼‰"
        )
    )


def verify_test_coverage() -> VerificationResult:
    """QA-001: ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ >= 70%"""
    print("\nğŸ” Verifying: Test Coverage (QA-001)")

    success, stdout, stderr = run_command([
        "pytest", "--cov=actions", "--cov=scripts",
        "--cov-report=term", "--tb=no", "-q"
    ])

    # Parse coverage percentage
    lines = stdout.split('\n')
    coverage_line = [l for l in lines if 'coverage:' in l.lower() or '%' in l]

    if not coverage_line:
        return VerificationResult(
            function_id="QA-001",
            scenario_name="ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ >= 70%",
            passed=False,
            actual_output=stdout,
            expected_output="Coverage: >= 70%",
            error_message="Could not find coverage in output",
            interpretation="ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®šã«å¤±æ•—"
        )

    # Extract coverage percentage
    # Format: "TOTAL                             927      65  92.99%"
    for line in coverage_line:
        if 'TOTAL' in line:
            parts = line.split()
            if len(parts) >= 4:
                try:
                    coverage_pct = float(parts[-1].rstrip('%'))
                    passed = coverage_pct >= 70.0

                    return VerificationResult(
                        function_id="QA-001",
                        scenario_name="ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ >= 70%",
                        passed=passed,
                        actual_output=f"{coverage_pct}%",
                        expected_output=">= 70%",
                        interpretation=(
                            f"ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸{coverage_pct}%ã¯ç›®æ¨™ã®70%ã‚’{'å¤§å¹…ã«' if coverage_pct >= 90 else ''}ä¸Šå›ã£ã¦ã„ã‚‹"
                            if passed
                            else f"ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸{coverage_pct}%ãŒç›®æ¨™ã®70%ã‚’ä¸‹å›ã£ã¦ã„ã‚‹"
                        )
                    )
                except (ValueError, IndexError):
                    pass

    return VerificationResult(
        function_id="QA-001",
        scenario_name="ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ >= 70%",
        passed=False,
        actual_output=stdout,
        expected_output=">= 70%",
        error_message="Could not parse coverage percentage",
        interpretation="ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—"
    )


def verify_integration_test_coverage() -> VerificationResult:
    """QA-003: Integration Test Coverage 100%"""
    print("\nğŸ” Verifying: Integration Test Coverage (QA-003)")

    repo_root = Path.cwd()
    integration_test_dir = repo_root / "tests" / "integration"

    if not integration_test_dir.exists():
        return VerificationResult(
            function_id="QA-003",
            scenario_name="Integration Test Coverage 100%",
            passed=False,
            actual_output="tests/integration/ directory not found",
            expected_output="13 integration test files",
            error_message="Integration test directory missing",
            interpretation="Integration testãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„"
        )

    # Count integration test files
    test_files = list(integration_test_dir.glob("test_*_integration.py"))
    test_count = len(test_files)

    passed = test_count >= 13

    return VerificationResult(
        function_id="QA-003",
        scenario_name="Integration Test Coverage 100%",
        passed=passed,
        actual_output=f"{test_count} integration test files",
        expected_output="13 integration test files",
        interpretation=(
            f"å…¨13å€‹ã®Actionã«å¯¾ã—ã¦Integration TestãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ï¼ˆ{test_count}/13ï¼‰"
            if passed
            else f"Integration TestãŒä¸è¶³ã—ã¦ã„ã‚‹ï¼ˆ{test_count}/13ï¼‰"
        )
    )


def verify_documentation_coverage() -> VerificationResult:
    """DOC-001: å…¨Actionsã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå­˜åœ¨"""
    print("\nğŸ” Verifying: Documentation Coverage (DOC-001)")

    repo_root = Path.cwd()

    # Count example files
    examples_dir = repo_root / "examples"
    example_files = list(examples_dir.glob("*-example.yml")) if examples_dir.exists() else []
    example_count = len(example_files)

    # Count instruction files
    instructions_dir = repo_root / "instructions"
    instruction_files = list(instructions_dir.glob("*.md")) if instructions_dir.exists() else []
    instruction_count = len(instruction_files)

    passed = example_count >= 13 and instruction_count >= 13

    return VerificationResult(
        function_id="DOC-001",
        scenario_name="å…¨Actionsã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå­˜åœ¨",
        passed=passed,
        actual_output=f"Examples: {example_count}, Instructions: {instruction_count}",
        expected_output="Examples: >= 13, Instructions: >= 13",
        interpretation=(
            f"å…¨Actionã«exampleãƒ•ã‚¡ã‚¤ãƒ«ã¨instructionãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå­˜åœ¨ã™ã‚‹"
            if passed
            else f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒä¸è¶³ã—ã¦ã„ã‚‹ï¼ˆExamples: {example_count}/13, Instructions: {instruction_count}/13ï¼‰"
        )
    )


def verify_ai_review_metrics() -> VerificationResult:
    """QA-002: AIãƒ¬ãƒ“ãƒ¥ãƒ¼å—å…¥ç‡ >= 70%"""
    print("\nğŸ” Verifying: AI Review Acceptance Rate (QA-002)")

    repo_root = Path.cwd()
    metrics_file = repo_root / "metrics" / "review_metrics.json"

    if not metrics_file.exists():
        return VerificationResult(
            function_id="QA-002",
            scenario_name="AIãƒ¬ãƒ“ãƒ¥ãƒ¼å—å…¥ç‡ >= 70%",
            passed=False,
            actual_output="metrics/review_metrics.json not found",
            expected_output="Acceptance rate >= 70%",
            error_message="No review metrics data available",
            interpretation=(
                "CRITICAL: AIãƒ¬ãƒ“ãƒ¥ãƒ¼å“è³ªã®æ ¸å¿ƒçš„æŒ‡æ¨™ãŒæ¸¬å®šã§ããªã„ã€‚\n"
                "metrics/review_metrics.json ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€å—å…¥ç‡ã‚’è¨ˆæ¸¬ã§ããªã„ã€‚\n"
                "å³æ™‚ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå¿…è¦ï¼šãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹"
            )
        )

    # Try to calculate acceptance rate
    success, stdout, stderr = run_command([
        "python", "scripts/calculate_acceptance_rate.py",
        "--time-period", "30d"
    ])

    if not success:
        return VerificationResult(
            function_id="QA-002",
            scenario_name="AIãƒ¬ãƒ“ãƒ¥ãƒ¼å—å…¥ç‡ >= 70%",
            passed=False,
            actual_output=stdout or stderr,
            expected_output="Acceptance rate >= 70%",
            error_message="Failed to calculate acceptance rate",
            interpretation="å—å…¥ç‡è¨ˆç®—ã«å¤±æ•—ã€‚ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§"
        )

    # Parse output (simplified)
    if "N/A" in stdout or "insufficient data" in stdout.lower():
        return VerificationResult(
            function_id="QA-002",
            scenario_name="AIãƒ¬ãƒ“ãƒ¥ãƒ¼å—å…¥ç‡ >= 70%",
            passed=False,
            actual_output="Insufficient data",
            expected_output="Acceptance rate >= 70%",
            error_message="Not enough review data",
            interpretation="ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ã€‚20ä»¶ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦"
        )

    # Extract acceptance rate (implementation would parse the actual output)
    # For now, assume failure
    return VerificationResult(
        function_id="QA-002",
        scenario_name="AIãƒ¬ãƒ“ãƒ¥ãƒ¼å—å…¥ç‡ >= 70%",
        passed=False,
        actual_output=stdout[:200],
        expected_output="Acceptance rate >= 70%",
        error_message="Could not determine acceptance rate",
        interpretation="å—å…¥ç‡ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã€ã¾ãŸã¯ç›®æ¨™æœªé”"
    )


def main():
    """Main verification entry point"""

    print("=" * 70)
    print("CORE FUNCTION VERIFICATION REPORT")
    print("=" * 70)
    print("Repo Genesis Auditor v2.0")
    print("Repository: github-actions-actions")
    print("Run ID: 2026-02-08T04:40:00Z")
    print("=" * 70)

    # Run all verifications
    results = [
        verify_action_structure(),
        verify_test_coverage(),
        verify_integration_test_coverage(),
        verify_documentation_coverage(),
        verify_ai_review_metrics(),
    ]

    # Print results
    print("\n" + "=" * 70)
    print("VERIFICATION RESULTS")
    print("=" * 70)

    passed_count = sum(1 for r in results if r.passed)
    total_count = len(results)

    for r in results:
        status = "âœ… PASS" if r.passed else "âŒ FAIL"
        print(f"\n{status} [{r.function_id}] {r.scenario_name}")
        print(f"  çµæœ: {r.actual_output}")
        print(f"  æœŸå¾…: {r.expected_output}")
        print(f"  è§£é‡ˆ: {r.interpretation}")
        if r.error_message:
            print(f"  ã‚¨ãƒ©ãƒ¼: {r.error_message}")

    # Summary
    print("\n" + "=" * 70)
    print(f"SUMMARY: {passed_count}/{total_count} passed ({passed_count/total_count*100:.1f}%)")
    print("=" * 70)

    if passed_count == total_count:
        print("âœ… åˆ¤å®š: PASS - å…¨ã¦ã®æ¤œè¨¼ã«åˆæ ¼")
        print("\nãƒªãƒã‚¸ãƒˆãƒªã®å­˜åœ¨æ„ç¾©ãŒæ¤œè¨¼ã•ã‚ŒãŸ")
        return 0
    elif passed_count >= total_count * 0.7:
        print("âš ï¸  åˆ¤å®š: CONDITIONAL PASS - ä¸€éƒ¨ã®æ¤œè¨¼ãŒä¸åˆæ ¼")
        print("\næ”¹å–„ãŒå¿…è¦ãªé ˜åŸŸ:")
        for r in results:
            if not r.passed:
                print(f"  - {r.function_id}: {r.interpretation.split('.')[0]}")
        print("\næ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print("  1. å¤±æ•—ã—ãŸæ¤œè¨¼é …ç›®ã®æ”¹å–„ææ¡ˆã‚’ç¢ºèªï¼ˆ.audit/proposal/changes/ï¼‰")
        print("  2. å„ªå…ˆåº¦ã®é«˜ã„å•é¡Œã‹ã‚‰å¯¾é–‹")
        return 0
    else:
        print("âŒ åˆ¤å®š: FAIL - å¤šãã®æ¤œè¨¼ãŒä¸åˆæ ¼")
        print("\næ·±åˆ»ãªå•é¡ŒãŒå­˜åœ¨ã™ã‚‹")
        return 1


if __name__ == "__main__":
    sys.exit(main())
