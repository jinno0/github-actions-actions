#!/usr/bin/env python3
"""
Core Function Verification Script
Generated by Repo Genesis Auditor v2.0

This script verifies that all 13 AI Actions can execute in Dry Run mode.
"""
import json
import subprocess
import sys
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Any


@dataclass
class VerificationResult:
    function_id: str
    function_name: str
    scenario_name: str
    passed: bool
    actual_output: Any
    expected_output: Any
    error_message: str | None = None
    interpretation: str = ""


def run_command(cmd: list[str], timeout: int = 30) -> tuple[bool, str, str]:
    """Run command and return (success, stdout, stderr)"""
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=timeout,
            cwd=Path(__file__).parent.parent.parent
        )
        return result.returncode == 0, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return False, "", "Command timed out"
    except Exception as e:
        return False, "", str(e)


def verify_action_structure(action_name: str) -> VerificationResult:
    """Verify that an action has required files"""
    base_path = Path(__file__).parent.parent.parent / "actions" / action_name

    required_files = {
        "action.yml": base_path / "action.yml",
        "README.md": base_path / "README.md"
    }

    missing_files = [
        f for f, path in required_files.items()
        if not path.exists()
    ]

    if missing_files:
        return VerificationResult(
            function_id=f"CF-{action_name}",
            function_name=action_name,
            scenario_name="Action Structure Check",
            passed=False,
            actual_output={"missing_files": missing_files},
            expected_output={"all_files_present": True},
            error_message=f"Missing files: {', '.join(missing_files)}",
            interpretation=f"Action {action_name} is incomplete"
        )

    return VerificationResult(
        function_id=f"CF-{action_name}",
        function_name=action_name,
        scenario_name="Action Structure Check",
        passed=True,
        actual_output={"files_present": list(required_files.keys())},
        expected_output={"all_files_present": True},
        interpretation=f"Action {action_name} has all required files"
    )


def verify_dry_run_execution(action_name: str) -> VerificationResult:
    """Verify that action tests pass (includes dry run validation)"""
    test_path = Path(__file__).parent.parent.parent / f"tests/test_{action_name.replace('-', '_')}/"

    if not test_path.exists():
        # Some actions might have different test directory naming
        test_path = Path(__file__).parent.parent.parent / "tests/integration/"
        test_pattern = f"test_{action_name.replace('-', '_')}"
    else:
        test_pattern = ""

    cmd = [
        "python", "-m", "pytest",
        str(test_path),
        "-v",
        "--tb=short",
        "-k", test_pattern or action_name.replace("-", "_")
    ]

    success, stdout, stderr = run_command(cmd, timeout=60)

    # Check if tests passed
    if success:
        return VerificationResult(
            function_id=f"CF-{action_name}",
            function_name=action_name,
            scenario_name="Dry Run Execution Test",
            passed=True,
            actual_output={"test_output": stdout[-500:] if stdout else ""},  # Last 500 chars
            expected_output={"tests": "passed"},
            interpretation=f"Action {action_name} tests pass successfully"
        )

    # Check if it's just test collection issues or actual failures
    if "collected" in stdout or "error" in stdout.lower():
        # Try running integration tests instead
        int_cmd = [
            "python", "-m", "pytest",
            str(Path(__file__).parent.parent.parent / "tests/integration/"),
            "-v",
            "--tb=short",
            "-k", action_name.replace("-", "_")
        ]
        success, stdout, stderr = run_command(int_cmd, timeout=60)

        if success:
            return VerificationResult(
                function_id=f"CF-{action_name}",
                function_name=action_name,
                scenario_name="Integration Test (Dry Run)",
                passed=True,
                actual_output={"test_output": stdout[-500:] if stdout else ""},
                expected_output={"tests": "passed"},
                interpretation=f"Action {action_name} integration tests pass"
            )

    return VerificationResult(
        function_id=f"CF-{action_name}",
        function_name=action_name,
        scenario_name="Test Execution",
        passed=False,
        actual_output={"stdout": stdout, "stderr": stderr},
        expected_output={"tests": "passed"},
        error_message=stderr or stdout,
        interpretation=f"Action {action_name} test execution failed"
    )


def main():
    """Verify all 13 Core Functions"""

    actions = [
        "review-and-merge",
        "spec-to-code",
        "action-fixer",
        "auto-refactor",
        "auto-document",
        "release-notes-ai",
        "auto-merge",
        "auto-rebase",
        "review-auto-merge",
        "bulk-merge-prs",
        "bulk-rebase-prs",
        "pr-review-enqueuer",
        "publish-pr"
    ]

    results = []

    print("=" * 80)
    print("CORE FUNCTION VERIFICATION REPORT")
    print("=" * 80)
    print()

    # First, verify action structure for all actions
    print("Phase 1: Verifying Action Structure...")
    print("-" * 80)

    structure_results = []
    for action in actions:
        result = verify_action_structure(action)
        structure_results.append(result)
        status = "✅ PASS" if result.passed else "❌ FAIL"
        print(f"{status} [{result.function_id}] {result.function_name}")
        if not result.passed:
            print(f"  → {result.error_message}")

    print()
    print(f"Structure Check: {sum(1 for r in structure_results if r.passed)}/{len(structure_results)} passed")
    print()

    # Second, run dry run tests
    print("Phase 2: Running Dry Run Tests...")
    print("-" * 80)

    test_results = []
    for i, action in enumerate(actions, 1):
        print(f"[{i}/{len(actions)}] Testing {action}...")
        result = verify_dry_run_execution(action)
        test_results.append(result)
        status = "✅" if result.passed else "❌"
        print(f"  {status} {result.scenario_name}: {result.interpretation}")

    results = structure_results + test_results

    # Generate summary
    print()
    print("=" * 80)
    print("SUMMARY")
    print("=" * 80)

    structure_passed = sum(1 for r in structure_results if r.passed)
    test_passed = sum(1 for r in test_results if r.passed)

    print(f"Structure Check: {structure_passed}/{len(structure_results)} passed")
    print(f"Execution Tests: {test_passed}/{len(test_results)} passed")
    print()

    # Overall verdict
    structure_pass_rate = structure_passed / len(structure_results) * 100
    test_pass_rate = test_passed / len(test_results) * 100

    if structure_pass_rate == 100 and test_pass_rate >= 95:  # Allow some test flakiness
        print("Overall Verdict: ✅ CORE FUNCTIONS VERIFIED")
        print()
        print("All 13 AI Actions have proper structure and most tests pass.")
        print("The repository's core functionality is validated.")
        exit_code = 0
    else:
        print("Overall Verdict: ⚠️  PARTIAL VERIFICATION")
        print()
        if structure_pass_rate < 100:
            print(f"⚠️  {len(structure_results) - structure_passed} action(s) missing required files")
        if test_pass_rate < 95:
            print(f"⚠️  {len(test_results) - test_passed} action(s) have test failures")
        exit_code = 1

    # Save results to JSON
    output_path = Path(__file__).parent.parent / "output" / "verification_result.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert dataclass to dict for JSON serialization
    results_dict = []
    for r in results:
        r_dict = asdict(r)
        # Handle non-serializable objects
        if isinstance(r_dict.get("actual_output"), dict):
            for key, value in r_dict["actual_output"].items():
                if isinstance(value, Path):
                    r_dict["actual_output"][key] = str(value)
        results_dict.append(r_dict)

    output_path.write_text(
        json.dumps(results_dict, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

    print()
    print(f"Results saved to: {output_path}")

    sys.exit(exit_code)


if __name__ == "__main__":
    main()
