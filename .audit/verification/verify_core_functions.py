#!/usr/bin/env python3
"""
Core Function Verification Script

Generated by Repo Genesis Auditor
Tests the essential functionality of the github-actions-actions repository.

Usage:
    python .audit/verification/verify_core_functions.py
"""
import json
import subprocess
import sys
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Any, Optional

@dataclass
class VerificationResult:
    """Result of a single verification test"""
    function_id: str
    scenario_name: str
    passed: bool
    actual_output: Any
    expected_output: Any
    error_message: Optional[str] = None
    interpretation: str = ""
    execution_time_ms: float = 0.0

class CoreFunctionVerifier:
    """Verifies core functions of the repository"""

    def __init__(self, repo_root: Path):
        self.repo_root = repo_root
        self.audit_dir = repo_root / ".audit"
        self.test_data_dir = self.audit_dir / "verification" / "test_data"
        self.results = []

    def verify_all(self) -> list[VerificationResult]:
        """Run all verification tests"""
        print("=" * 70)
        print("CORE FUNCTION VERIFICATION")
        print("=" * 70)
        print(f"Repository: {self.repo_root}")
        print(f"Test Data: {self.test_data_dir}")
        print()

        # Create test data directory
        self.test_data_dir.mkdir(parents=True, exist_ok=True)

        # Verify each core function
        self.verify_cf006_action_fixer()
        self.verify_cf002_spec_to_code()
        self.verify_cf004_auto_document()
        self.verify_cf003_auto_refactor()

        return self.results

    def verify_cf006_action_fixer(self):
        """CF-006: action-fixer - Fix workflow syntax errors"""
        print("\n" + "=" * 70)
        print("CF-006: action-fixer")
        print("=" * 70)

        # Scenario 1: Fix indentation error
        result = self._test_action_fixer_indentation()
        self.results.append(result)
        self._print_result(result)

        # Scenario 2: Validate action.yml files exist
        result = self._test_action_fixer_validate_files()
        self.results.append(result)
        self._print_result(result)

    def _test_action_fixer_indentation(self) -> VerificationResult:
        """Test that action-fixer can detect YAML syntax errors"""
        import time
        start = time.time()

        # Create test file with bad indentation
        test_file = self.test_data_dir / "bad_indent.yml"
        test_file.write_text("""name: Test
on: push
jobs:
  build:
  runs-on: ubuntu-latest
""")

        try:
            # Try to validate the YAML (simulate action-fixer behavior)
            import yaml
            try:
                yaml.safe_load(test_file.read_text())
                passed = False
                interpretation = "YAML was loaded without error (unexpected)"
                actual = "valid_yaml"
            except yaml.YAMLError as e:
                passed = True
                interpretation = "Correctly detected YAML syntax error"
                actual = f"yaml_error: {str(e)[:100]}"

            return VerificationResult(
                function_id="CF-006",
                scenario_name="Detect YAML indentation error",
                passed=passed,
                actual_output=actual,
                expected_output="yaml_error",
                interpretation=interpretation,
                execution_time_ms=(time.time() - start) * 1000
            )

        except Exception as e:
            return VerificationResult(
                function_id="CF-006",
                scenario_name="Detect YAML indentation error",
                passed=False,
                actual_output=None,
                expected_output="yaml_error",
                error_message=str(e),
                interpretation="YAML validation library not available or test failed",
                execution_time_ms=(time.time() - start) * 1000
            )

    def _test_action_fixer_validate_files(self) -> VerificationResult:
        """Test that action.yml files exist and are valid"""
        import time
        start = time.time()

        actions_dir = self.repo_root / "actions"
        action_yml_files = list(actions_dir.rglob("action.yml"))

        passed = len(action_yml_files) >= 6  # At least 6 core actions
        interpretation = f"Found {len(action_yml_files)} action.yml files (expected >= 6)"

        return VerificationResult(
            function_id="CF-006",
            scenario_name="Validate action.yml files exist",
            passed=passed,
            actual_output=len(action_yml_files),
            expected_output=">= 6",
            interpretation=interpretation,
            execution_time_ms=(time.time() - start) * 1000
        )

    def verify_cf002_spec_to_code(self):
        """CF-002: spec-to-code - Generate code from specifications"""
        print("\n" + "=" * 70)
        print("CF-002: spec-to-code")
        print("=" * 70)

        # Scenario 1: Test spec file exists
        result = self._test_spec_to_code_has_examples()
        self.results.append(result)
        self._print_result(result)

        # Scenario 2: Test action.yml has required inputs
        result = self._test_spec_to_code_action_config()
        self.results.append(result)
        self._print_result(result)

    def _test_spec_to_code_has_examples(self) -> VerificationResult:
        """Test that spec-to-code has example workflows"""
        import time
        start = time.time()

        examples_dir = self.repo_root / "examples"
        spec_examples = list(examples_dir.glob("*spec-to-code*"))

        passed = len(spec_examples) > 0
        interpretation = f"Found {len(spec_examples)} spec-to-code examples"

        return VerificationResult(
            function_id="CF-002",
            scenario_name="Has example workflows",
            passed=passed,
            actual_output=len(spec_examples),
            expected_output=">= 1",
            interpretation=interpretation,
            execution_time_ms=(time.time() - start) * 1000
        )

    def _test_spec_to_code_action_config(self) -> VerificationResult:
        """Test that spec-to-code action.yml has required inputs"""
        import time
        import yaml
        start = time.time()

        action_yml = self.repo_root / "actions" / "spec-to-code" / "action.yml"

        if not action_yml.exists():
            return VerificationResult(
                function_id="CF-002",
                scenario_name="Action configuration",
                passed=False,
                actual_output="action.yml not found",
                expected_output="action.yml exists",
                interpretation="Critical: action.yml is missing",
                execution_time_ms=(time.time() - start) * 1000
            )

        try:
            config = yaml.safe_load(action_yml.read_text())
            inputs = config.get("inputs", {})

            required_inputs = ["spec-path", "output-dir", "language"]
            has_required = all(inp in inputs for inp in required_inputs)

            passed = has_required
            actual_inputs = list(inputs.keys())
            interpretation = f"Action has {len(actual_inputs)} inputs: {', '.join(actual_inputs[:5])}"

            return VerificationResult(
                function_id="CF-002",
                scenario_name="Action configuration",
                passed=passed,
                actual_output=actual_inputs,
                expected_output=required_inputs,
                interpretation=interpretation,
                execution_time_ms=(time.time() - start) * 1000
            )

        except Exception as e:
            return VerificationResult(
                function_id="CF-002",
                scenario_name="Action configuration",
                passed=False,
                actual_output=None,
                expected_output=required_inputs,
                error_message=str(e),
                interpretation="Failed to parse action.yml",
                execution_time_ms=(time.time() - start) * 1000
            )

    def verify_cf004_auto_document(self):
        """CF-004: auto-document - Automatic documentation updates"""
        print("\n" + "=" * 70)
        print("CF-004: auto-document")
        print("=" * 70)

        result = self._test_auto_document_has_examples()
        self.results.append(result)
        self._print_result(result)

    def _test_auto_document_has_examples(self) -> VerificationResult:
        """Test that auto-document has example workflows"""
        import time
        start = time.time()

        examples_dir = self.repo_root / "examples"
        doc_examples = list(examples_dir.glob("*auto-document*"))

        passed = len(doc_examples) > 0
        interpretation = f"Found {len(doc_examples)} auto-document examples"

        return VerificationResult(
            function_id="CF-004",
            scenario_name="Has example workflows",
            passed=passed,
            actual_output=len(doc_examples),
            expected_output=">= 1",
            interpretation=interpretation,
            execution_time_ms=(time.time() - start) * 1000
        )

    def verify_cf003_auto_refactor(self):
        """CF-003: auto-refactor - Intelligent code refactoring"""
        print("\n" + "=" * 70)
        print("CF-003: auto-refactor")
        print("=" * 70)

        result = self._test_auto_refactor_has_examples()
        self.results.append(result)
        self._print_result(result)

    def _test_auto_refactor_has_examples(self) -> VerificationResult:
        """Test that auto-refactor has example workflows"""
        import time
        start = time.time()

        examples_dir = self.repo_root / "examples"
        refactor_examples = list(examples_dir.glob("*auto-refactor*"))

        passed = len(refactor_examples) > 0
        interpretation = f"Found {len(refactor_examples)} auto-refactor examples"

        return VerificationResult(
            function_id="CF-003",
            scenario_name="Has example workflows",
            passed=passed,
            actual_output=len(refactor_examples),
            expected_output=">= 1",
            interpretation=interpretation,
            execution_time_ms=(time.time() - start) * 1000
        )

    def _print_result(self, result: VerificationResult):
        """Print a verification result"""
        status = "✅ PASS" if result.passed else "❌ FAIL"
        print(f"\n{status} [{result.function_id}] {result.scenario_name}")
        print(f"  解釈: {result.interpretation}")
        if result.error_message:
            print(f"  エラー: {result.error_message}")
        print(f"  実行時間: {result.execution_time_ms:.2f}ms")

    def generate_report(self) -> dict:
        """Generate summary report"""
        passed_count = sum(1 for r in self.results if r.passed)
        total_count = len(self.results)

        summary = {
            "run_id": "20260208-151100",
            "timestamp": "2026-02-08T15:11:00Z",
            "total_tests": total_count,
            "passed": passed_count,
            "failed": total_count - passed_count,
            "pass_rate": f"{(passed_count / total_count * 100):.1f}%" if total_count > 0 else "N/A",
            "results": [asdict(r) for r in self.results],
            "overall_status": "PASS" if passed_count == total_count else "FAIL" if passed_count == 0 else "PARTIAL"
        }

        return summary

def main():
    """Main entry point"""
    # Determine repo root (assuming we're in .audit/verification/)
    script_path = Path(__file__).resolve()
    repo_root = script_path.parent.parent.parent

    verifier = CoreFunctionVerifier(repo_root)
    verifier.verify_all()

    # Generate and print summary
    print("\n" + "=" * 70)
    print("VERIFICATION SUMMARY")
    print("=" * 70)

    summary = verifier.generate_report()
    print(f"\nTotal Tests: {summary['total_tests']}")
    print(f"Passed: {summary['passed']}")
    print(f"Failed: {summary['failed']}")
    print(f"Pass Rate: {summary['pass_rate']}")

    if summary['overall_status'] == "PASS":
        print("\n判定: ✅ リポジトリの核心機能が検証された")
    elif summary['overall_status'] == "PARTIAL":
        print("\n判定: ⚠️ 一部の機能に問題がある")
        print("\n失敗したテスト:")
        for r in verifier.results:
            if not r.passed:
                print(f"  - {r.function_id}: {r.interpretation}")
    else:
        print("\n判定: ❌ 核心機能の検証に失敗")

    # Save results
    output_path = Path(".audit/output/verification_result.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(summary, indent=2, ensure_ascii=False))
    print(f"\n結果を保存: {output_path}")

    sys.exit(0 if summary['overall_status'] == "PASS" else 1)

if __name__ == "__main__":
    main()
