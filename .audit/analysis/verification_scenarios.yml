version: "2.0"
run_id: "20260208-151100"
generated_at: "2026-02-08T15:11:00Z"

# === Core Function Verification Scenarios ===

## CF-001: review-and-merge

### Scenario 1: Basic PR Review
**Name:** PR has simple code change
**Input:**
  type: "pull_request"
  data:
    title: "Fix typo in README"
    files_changed: ["README.md"]
    diff: "- Hellp World\n+ Hello World"
**Expected Output:**
  type: "review_comment"
  contains: ["LGTM", "Approved"]
  confidence_score: ">= 8"
**Edge Cases:**
  - Empty PR
  - PR with only whitespace changes
  - PR with 1000+ files

### Scenario 2: PR with Issues
**Name:** PR has code quality issues
**Input:**
  type: "pull_request"
  data:
    title: "Add new feature"
    files_changed: ["src/main.py"]
    issues: ["missing-type-hints", "no-docstring"]
**Expected Output:**
  type: "review_comment"
  contains: ["requested changes", "type hints", "docstring"]
  confidence_score: "< 7"
  auto_fix_suggested: true

### Scenario 3: Auto-fix Trigger
**Name:** PR triggers auto-fix
**Input:**
  type: "pull_request"
  config:
    auto_fix: true
    lgtm_threshold: 7
  data:
    issues_severity: "low"
**Expected Output:**
  type: "commit"
  action: "auto_fix_applied"
  result: "issues_fixed"

---

## CF-002: spec-to-code

### Scenario 1: Python Function Generation
**Name:** Generate Python function from spec
**Input:**
  type: "markdown_spec"
  file: ".audit/test_data/spec_python.md"
  content: |
    # Spec: Calculate Fibonacci

    ## Requirements
    - Function: `fibonacci(n: int) -> int`
    - Returns nth Fibonacci number
    - Input validation: n must be >= 0
**Expected Output:**
  type: "python_code"
  language: "python"
  contains:
    - "def fibonacci(n: int) -> int:"
    - "if n < 0:"
    - "raise ValueError"
  syntactically_valid: true
  passes_type_check: true

### Scenario 2: TypeScript Interface Generation
**Name:** Generate TypeScript from spec
**Input:**
  type: "markdown_spec"
  content: |
    # User Interface

    ## Properties
    - id: string
    - name: string
    - email: string
**Expected Output:**
  type: "typescript_code"
  contains: ["interface User", "id: string", "name: string"]

### Scenario 3: Invalid Spec
**Name:** Handle malformed spec
**Input:**
  type: "markdown_spec"
  content: "This is not a valid spec\nNo requirements here"
**Expected Output:**
  type: "error"
  error_message: "spec validation failed"
  exit_code: 1

---

## CF-003: auto-refactor

### Scenario 1: Improve Readability
**Name:** Refactor for readability
**Input:**
  type: "python_file"
  file: ".audit/test_data/messy_code.py"
  content: |
    def f(x):return x*2+1
**Instruction:** "Improve readability and add type hints"
**Expected Output:**
  type: "python_code"
  contains: ["def calculate(", "x: int", "-> int"]
  improvements:
    - "added_type_hints"
    - "improved_naming"
    - "added_whitespace"

### Scenario 2: Update to Modern Patterns
**Name:** Migrate old Python code
**Input:**
  type: "python_file"
  content: |
    class OldClass(object):
      def __init__(self, value):
          self.value = value
**Instruction:** "Update to Python 3.11+ patterns"
**Expected Output:**
  type: "python_code"
  improvements:
    - "removed_object_base"
    - "used_dataclass"

### Scenario 3: Safe Refactoring
**Name:** Preserve behavior
**Input:**
  type: "python_file"
  file: ".audit/test_data/critical_logic.py"
**Instruction:** "Refactor for clarity"
**Expected Output:**
  type: "python_code"
  behavior_preserved: true
  tests_still_pass: true

---

## CF-004: auto-document

### Scenario 1: Generate Function Docstring
**Name:** Add docstrings to undocumented code
**Input:**
  type: "python_file"
  content: |
    def calculate_discount(price, discount_rate):
        return price * (1 - discount_rate)
**Expected Output:**
  type: "python_code"
  contains: ['"""', "Args:", "Returns:", "price", "discount_rate"]

### Scenario 2: Update README
**Name:** Sync README with code changes
**Input:**
  type: "codebase"
  changes:
    - "New function added: `export_data()`"
    - "Removed function: `legacy_import()`"
  source_path: "src/"
  doc_path: "README.md"
**Expected Output:**
  type: "markdown_document"
  updated_sections: ["API Reference", "Usage Examples"]
  removed_references: ["legacy_import"]

### Scenario 3: Generate JSDoc
**Name:** Document JavaScript/TypeScript code
**Input:**
  type: "typescript_file"
  content: |
    function createUser(name, email) {
      return { name, email };
    }
**Expected Output:**
  type: "typescript_code"
  contains: ["/**", "@param", "@returns"]

---

## CF-005: release-notes-ai

### Scenario 1: Standard Release Notes
**Name:** Generate notes for tag range
**Input:**
  type: "git_range"
  since_tag: "v1.0.0"
  until_tag: "v1.1.0"
  commits:
    - "feat: add user authentication"
    - "fix: resolve login timeout issue"
    - "docs: update README"
**Expected Output:**
  type: "markdown_document"
  sections:
    - "## üöÄ Features"
    - "## üêõ Bug Fixes"
    - "## üìù Documentation"
  categorized_commits: true
  summarization_quality: "high"

### Scenario 2: Breaking Changes Highlight
**Name:** Detect and highlight breaking changes
**Input:**
  commits:
    - "feat!: change API signature"
    - "BREAKING CHANGE: remove deprecated endpoint"
**Expected Output:**
  type: "markdown_document"
  contains: ["## üí• Breaking Changes", "API signature", "migration guide"]

### Scenario 3: Empty Range
**Name:** Handle no commits between tags
**Input:**
  since_tag: "v1.0.0"
  until_tag: "v1.0.0"
  commits: []
**Expected Output:**
  type: "markdown_document"
  content: "No changes in this release"
  exit_code: 0

---

## CF-006: action-fixer

### Scenario 1: Fix Indentation Error
**Name:** Detect and fix YAML indentation
**Input:**
  type: "workflow_file"
  file: ".audit/test_data/bad_indent.yml"
  content: |
    name: Test
    on: push
    jobs:
      build:
      runs-on: ubuntu-latest
    # Wrong indentation above
**Expected Output:**
  type: "yaml_file"
  fixed_issues:
    - "indentation_error"
  syntactically_valid: true
  preserves_comments: true

### Scenario 2: Fix Invalid Action Reference
**Name:** Correct malformed action reference
**Input:**
  type: "workflow_file"
  content: |
    steps:
      - uses: actions/checkout
      # Missing @v4
**Expected Output:**
  type: "yaml_file"
  fixed_issues:
    - "missing_version_reference"
  corrected_to: "actions/checkout@v4"

### Scenario 3: Validate with Schema
**Name:** Ensure action.yml is valid
**Input:**
  type: "action_metadata"
  file: "actions/example/action.yml"
**Expected Output:**
  type: "validation_result"
  schema_valid: true
  required_fields_present:
    - "name"
    - "description"
    - "runs"
  errors: []

---

## Test Data Files

### Required Test Data Files

1. `.audit/test_data/spec_python.md`
2. `.audit/test_data/messy_code.py`
3. `.audit/test_data/critical_logic.py`
4. `.audit/test_data/bad_indent.yml`
5. `.audit/test_data/pr_simple.diff`
6. `.audit/test_data/pr_with_issues.diff`

### Generation Method

Most test data should be manually created to cover specific edge cases.
Some can be generated using:
- `faker` library for realistic data
- Sampling from actual PRs (sanitized)
- Synthetic cases based on common bugs

---

## Verification Execution Plan

### Priority Order

1. **CF-006 (action-fixer):** Easiest to test, no external dependencies
2. **CF-002 (spec-to-code):** High value, testable with local execution
3. **CF-004 (auto-document):** Medium value, requires file I/O
4. **CF-003 (auto-refactor):** High value, requires careful validation
5. **CF-001 (review-and-merge):** Highest value, requires GitHub API
6. **CF-005 (release-notes-ai):** Medium value, requires git history

### Verification Method

For each core function:
1. **Unit Test Level:** Test individual scenarios
2. **Integration Test Level:** Test action.yml ‚Üí script execution
3. **Acceptance Test Level:** Test full workflow

### Success Criteria

A core function is "verified" when:
- ‚úÖ All scenarios pass
- ‚úÖ Edge cases are handled
- ‚úÖ Exit codes are correct
- ‚úÖ Output format matches specification
- ‚úÖ No crashes or hangs

---

## Limitations

### What We CANNOT Verify in This Audit

1. **Claude CLI Quality:** The actual AI output quality depends on Claude model
2. **Real-World Performance:** Test data is synthetic, may not match real usage
3. **GitHub API Integration:** Requires actual GitHub access token
4. **Self-Hosted Runner Environment:** Tests run in standard Ubuntu environment

### What We CAN Verify

1. ‚úÖ Script execution (no syntax errors)
2. ‚úÖ Input validation (rejects invalid input)
3. ‚úÖ Output structure (correct format)
4. ‚úÖ Error handling (graceful failures)
5. ‚úÖ Basic functionality (simple cases work)
